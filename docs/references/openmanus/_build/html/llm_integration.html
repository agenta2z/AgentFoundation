<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>LLM Integration &#8212; OpenManus 0.1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=12dfc556" />
    <script src="_static/documentation_options.js?v=01f34227"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Configuration" href="configuration.html" />
    <link rel="prev" title="Knowledge and Skills System" href="knowledge_and_skills.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="llm-integration">
<h1>LLM Integration<a class="headerlink" href="#llm-integration" title="Link to this heading">¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#overview" id="id1">Overview</a></p></li>
<li><p><a class="reference internal" href="#llm-client" id="id2">LLM Client</a></p>
<ul>
<li><p><a class="reference internal" href="#singleton-pattern" id="id3">Singleton Pattern</a></p></li>
<li><p><a class="reference internal" href="#configuration" id="id4">Configuration</a></p></li>
<li><p><a class="reference internal" href="#supported-providers" id="id5">Supported Providers</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#core-methods" id="id6">Core Methods</a></p>
<ul>
<li><p><a class="reference internal" href="#ask-text-generation" id="id7">ask() – Text Generation</a></p></li>
<li><p><a class="reference internal" href="#ask-with-images-multimodal" id="id8">ask_with_images() – Multimodal</a></p></li>
<li><p><a class="reference internal" href="#ask-tool-function-calling" id="id9">ask_tool() – Function Calling</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#message-formatting" id="id10">Message Formatting</a></p></li>
<li><p><a class="reference internal" href="#token-counting" id="id11">Token Counting</a></p></li>
<li><p><a class="reference internal" href="#retry-logic" id="id12">Retry Logic</a></p></li>
<li><p><a class="reference internal" href="#aws-bedrock-adapter" id="id13">AWS Bedrock Adapter</a></p></li>
<li><p><a class="reference internal" href="#prompt-management" id="id14">Prompt Management</a></p>
<ul>
<li><p><a class="reference internal" href="#prompt-templates" id="id15">Prompt Templates</a></p></li>
<li><p><a class="reference internal" href="#prompt-injection-points" id="id16">Prompt Injection Points</a></p></li>
<li><p><a class="reference internal" href="#dynamic-prompt-modification" id="id17">Dynamic Prompt Modification</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="overview">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Overview</a><a class="headerlink" href="#overview" title="Link to this heading">¶</a></h2>
<p>OpenManus abstracts LLM communication through a unified <code class="docutils literal notranslate"><span class="pre">LLM</span></code> class
that supports multiple providers (OpenAI, Azure OpenAI, AWS Bedrock)
with a consistent API. The LLM layer handles message formatting, tool
calling, token counting, retry logic, and streaming.</p>
</section>
<section id="llm-client">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">LLM Client</a><a class="headerlink" href="#llm-client" title="Link to this heading">¶</a></h2>
<p><strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">app/llm.py</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">LLM</span></code> class is a singleton-per-config-name wrapper around the
OpenAI SDK client.</p>
<section id="singleton-pattern">
<h3><a class="toc-backref" href="#id3" role="doc-backlink">Singleton Pattern</a><a class="headerlink" href="#singleton-pattern" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LLM</span><span class="p">:</span>
    <span class="n">_instances</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;LLM&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_instance</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LLM&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">config_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_instances</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instances</span><span class="p">[</span><span class="n">config_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">config_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_instances</span><span class="p">[</span><span class="n">config_name</span><span class="p">]</span>
</pre></div>
</div>
<p>This ensures one LLM client per configuration section, allowing
different agents to use different models (e.g., <code class="docutils literal notranslate"><span class="pre">[llm]</span></code> for the
main model, <code class="docutils literal notranslate"><span class="pre">[llm.vision]</span></code> for vision tasks).</p>
</section>
<section id="configuration">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Configuration</a><a class="headerlink" href="#configuration" title="Link to this heading">¶</a></h3>
<p>LLM settings come from the TOML config:</p>
<div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="k">[llm]</span>
<span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;gpt-4o&quot;</span>
<span class="n">base_url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;https://api.openai.com/v1&quot;</span>
<span class="n">api_key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;sk-...&quot;</span>
<span class="n">max_tokens</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4096</span>
<span class="n">temperature</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span>
<span class="n">timeout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">60</span>
<span class="n">max_retries</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span>
</pre></div>
</div>
</section>
<section id="supported-providers">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Supported Providers</a><a class="headerlink" href="#supported-providers" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 30.0%" />
<col style="width: 50.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Provider</p></th>
<th class="head"><p>Config</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>OpenAI</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">base_url</span> <span class="pre">=</span> <span class="pre">&quot;https://api.openai.com/v1&quot;</span></code></p></td>
<td><p>Default provider, full feature support</p></td>
</tr>
<tr class="row-odd"><td><p>Azure OpenAI</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">api_type</span> <span class="pre">=</span> <span class="pre">&quot;azure&quot;</span></code></p></td>
<td><p>Uses <code class="docutils literal notranslate"><span class="pre">AzureOpenAI</span></code> client, requires <code class="docutils literal notranslate"><span class="pre">api_version</span></code></p></td>
</tr>
<tr class="row-even"><td><p>AWS Bedrock</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">api_type</span> <span class="pre">=</span> <span class="pre">&quot;aws&quot;</span></code></p></td>
<td><p>Uses custom <code class="docutils literal notranslate"><span class="pre">BedrockClient</span></code> adapter</p></td>
</tr>
<tr class="row-odd"><td><p>Ollama</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">base_url</span> <span class="pre">=</span> <span class="pre">&quot;http://localhost:11434/v1&quot;</span></code></p></td>
<td><p>Local models via OpenAI-compatible API</p></td>
</tr>
<tr class="row-even"><td><p>Anthropic</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">base_url</span></code> pointing to Anthropic-compatible endpoint</p></td>
<td><p>Via OpenAI-compatible proxy</p></td>
</tr>
<tr class="row-odd"><td><p>Google</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">base_url</span></code> pointing to Google-compatible endpoint</p></td>
<td><p>Via OpenAI-compatible proxy</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="core-methods">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Core Methods</a><a class="headerlink" href="#core-methods" title="Link to this heading">¶</a></h2>
<section id="ask-text-generation">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">ask() – Text Generation</a><a class="headerlink" href="#ask-text-generation" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">ask</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span>
    <span class="n">system_msgs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate text response from LLM.&quot;&quot;&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Combines system messages with conversation messages</p></li>
<li><p>Supports streaming (default) and non-streaming modes</p></li>
<li><p>Returns the complete response text</p></li>
</ul>
</section>
<section id="ask-with-images-multimodal">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">ask_with_images() – Multimodal</a><a class="headerlink" href="#ask-with-images-multimodal" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">ask_with_images</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span>
    <span class="n">images</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">system_msgs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate response with image context.&quot;&quot;&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Injects base64-encoded images into the last user message</p></li>
<li><p>Uses the <code class="docutils literal notranslate"><span class="pre">[llm.vision]</span></code> model configuration if available</p></li>
</ul>
</section>
<section id="ask-tool-function-calling">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">ask_tool() – Function Calling</a><a class="headerlink" href="#ask-tool-function-calling" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">ask_tool</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span>
    <span class="n">system_msgs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tool_choice</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatCompletionMessage</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate response with tool/function calling.&quot;&quot;&quot;</span>
</pre></div>
</div>
<p>This is the primary method used by <code class="docutils literal notranslate"><span class="pre">ToolCallAgent.think()</span></code>:</p>
<ol class="arabic simple">
<li><p>Formats messages with system prompt</p></li>
<li><p>Calls OpenAI API with tool schemas</p></li>
<li><p>Returns the raw <code class="docutils literal notranslate"><span class="pre">ChatCompletionMessage</span></code> (may contain tool_calls)</p></li>
<li><p>The agent then parses tool_calls and dispatches to tools</p></li>
</ol>
<p>Parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tools</span></code>: List of tool schemas in OpenAI format (from
<code class="docutils literal notranslate"><span class="pre">ToolCollection.to_params()</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tool_choice</span></code>: <code class="docutils literal notranslate"><span class="pre">&quot;auto&quot;</span></code> (LLM decides), <code class="docutils literal notranslate"><span class="pre">&quot;required&quot;</span></code>
(must call a tool), <code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code> (text only)</p></li>
</ul>
</section>
</section>
<section id="message-formatting">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Message Formatting</a><a class="headerlink" href="#message-formatting" title="Link to this heading">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">format_messages()</span></code> method converts internal <code class="docutils literal notranslate"><span class="pre">Message</span></code> objects
to the OpenAI API format:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">format_messages</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Message</span><span class="p">],</span>
    <span class="n">system_msgs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert Message objects to OpenAI API format.&quot;&quot;&quot;</span>
</pre></div>
</div>
<p>Conversion rules:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Message.role</span></code> → <code class="docutils literal notranslate"><span class="pre">role</span></code> field</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Message.content</span></code> → <code class="docutils literal notranslate"><span class="pre">content</span></code> field</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Message.tool_calls</span></code> → <code class="docutils literal notranslate"><span class="pre">tool_calls</span></code> list with function name and
arguments</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Message.tool_call_id</span></code> → <code class="docutils literal notranslate"><span class="pre">tool_call_id</span></code> for tool result messages</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Message.base64_image</span></code> → injected as image_url content part</p></li>
</ul>
</section>
<section id="token-counting">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Token Counting</a><a class="headerlink" href="#token-counting" title="Link to this heading">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">LLM</span></code> class includes a <code class="docutils literal notranslate"><span class="pre">TokenCounter</span></code> for tracking API token
usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TokenCounter</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_input_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_output_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_input_tokens</span> <span class="o">+=</span> <span class="n">input_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_output_tokens</span> <span class="o">+=</span> <span class="n">output_tokens</span>
</pre></div>
</div>
<p>Token counting serves two purposes:</p>
<ol class="arabic simple">
<li><p><strong>Budget Management</strong>: Track total API costs across a session</p></li>
<li><p><strong>Context Window Management</strong>: Ensure messages fit within the
model’s context window</p></li>
</ol>
<p>The <code class="docutils literal notranslate"><span class="pre">tiktoken</span></code> library is used for local token estimation.</p>
</section>
<section id="retry-logic">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">Retry Logic</a><a class="headerlink" href="#retry-logic" title="Link to this heading">¶</a></h2>
<p>API calls use <code class="docutils literal notranslate"><span class="pre">tenacity</span></code> for retry with exponential backoff:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@retry</span><span class="p">(</span>
    <span class="n">wait</span><span class="o">=</span><span class="n">wait_random_exponential</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">60</span><span class="p">),</span>
    <span class="n">stop</span><span class="o">=</span><span class="n">stop_after_attempt</span><span class="p">(</span><span class="n">max_retries</span><span class="p">),</span>
    <span class="n">retry</span><span class="o">=</span><span class="n">retry_if_exception_type</span><span class="p">((</span>
        <span class="n">openai</span><span class="o">.</span><span class="n">RateLimitError</span><span class="p">,</span>
        <span class="n">openai</span><span class="o">.</span><span class="n">APITimeoutError</span><span class="p">,</span>
        <span class="n">openai</span><span class="o">.</span><span class="n">APIConnectionError</span><span class="p">,</span>
    <span class="p">)),</span>
<span class="p">)</span>
<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_call_api</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>This handles transient API failures gracefully, with configurable
<code class="docutils literal notranslate"><span class="pre">max_retries</span></code> (default: 3) and randomized exponential backoff.</p>
</section>
<section id="aws-bedrock-adapter">
<h2><a class="toc-backref" href="#id13" role="doc-backlink">AWS Bedrock Adapter</a><a class="headerlink" href="#aws-bedrock-adapter" title="Link to this heading">¶</a></h2>
<p><strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">app/bedrock.py</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">BedrockClient</span></code> adapts the AWS Bedrock Converse API to the
OpenAI message format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OpenAI Message Format ──► BedrockClient ──► Bedrock Converse API
                                                   │
OpenAI Response Format ◄── BedrockClient ◄──────────┘
</pre></div>
</div>
<p>Key conversions:</p>
<ul class="simple">
<li><p>OpenAI <code class="docutils literal notranslate"><span class="pre">role:</span> <span class="pre">&quot;assistant&quot;</span></code> → Bedrock <code class="docutils literal notranslate"><span class="pre">role:</span> <span class="pre">&quot;assistant&quot;</span></code></p></li>
<li><p>OpenAI <code class="docutils literal notranslate"><span class="pre">tool_calls</span></code> → Bedrock <code class="docutils literal notranslate"><span class="pre">toolUse</span></code> blocks</p></li>
<li><p>OpenAI <code class="docutils literal notranslate"><span class="pre">role:</span> <span class="pre">&quot;tool&quot;</span></code> → Bedrock <code class="docutils literal notranslate"><span class="pre">toolResult</span></code> blocks</p></li>
<li><p>Bedrock <code class="docutils literal notranslate"><span class="pre">converse()</span></code> response → OpenAI <code class="docutils literal notranslate"><span class="pre">ChatCompletionMessage</span></code></p></li>
</ul>
<p>This enables all OpenManus agents to work with AWS Bedrock models
(Claude, Llama, etc.) transparently.</p>
</section>
<section id="prompt-management">
<h2><a class="toc-backref" href="#id14" role="doc-backlink">Prompt Management</a><a class="headerlink" href="#prompt-management" title="Link to this heading">¶</a></h2>
<section id="prompt-templates">
<h3><a class="toc-backref" href="#id15" role="doc-backlink">Prompt Templates</a><a class="headerlink" href="#prompt-templates" title="Link to this heading">¶</a></h3>
<p>All prompts are static strings in <code class="docutils literal notranslate"><span class="pre">app/prompt/</span></code>:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 30.0%" />
<col style="width: 50.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>File</p></th>
<th class="head"><p>Constants</p></th>
<th class="head"><p>Used By</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">manus.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SYSTEM_PROMPT</span></code>, <code class="docutils literal notranslate"><span class="pre">NEXT_STEP_PROMPT</span></code></p></td>
<td><p>Manus, SandboxManus</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">toolcall.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SYSTEM_PROMPT</span></code>, <code class="docutils literal notranslate"><span class="pre">NEXT_STEP_PROMPT</span></code></p></td>
<td><p>ToolCallAgent base</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">browser.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SYSTEM_PROMPT</span></code>, <code class="docutils literal notranslate"><span class="pre">NEXT_STEP_PROMPT</span></code></p></td>
<td><p>BrowserAgent</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">swe.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SYSTEM_PROMPT</span></code></p></td>
<td><p>SWEAgent</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">planning.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">PLANNING_SYSTEM_PROMPT</span></code>, <code class="docutils literal notranslate"><span class="pre">NEXT_STEP_PROMPT</span></code></p></td>
<td><p>PlanningFlow</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">mcp.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SYSTEM_PROMPT</span></code>, <code class="docutils literal notranslate"><span class="pre">NEXT_STEP_PROMPT</span></code>,
<code class="docutils literal notranslate"><span class="pre">TOOL_ERROR_PROMPT</span></code>, <code class="docutils literal notranslate"><span class="pre">MULTIMEDIA_RESPONSE_PROMPT</span></code></p></td>
<td><p>MCPAgent</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">visualization.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SYSTEM_PROMPT</span></code>, <code class="docutils literal notranslate"><span class="pre">NEXT_STEP_PROMPT</span></code></p></td>
<td><p>DataAnalysis</p></td>
</tr>
</tbody>
</table>
</section>
<section id="prompt-injection-points">
<h3><a class="toc-backref" href="#id16" role="doc-backlink">Prompt Injection Points</a><a class="headerlink" href="#prompt-injection-points" title="Link to this heading">¶</a></h3>
<p>Prompts enter the LLM context at two points:</p>
<ol class="arabic simple">
<li><p><strong>System Prompt</strong>: Set once at agent creation, prepended to every
LLM call via <code class="docutils literal notranslate"><span class="pre">format_messages()</span></code></p></li>
<li><p><strong>Next Step Prompt</strong>: Appended as a user message before each
<code class="docutils literal notranslate"><span class="pre">think()</span></code> call, providing per-step context like current working
directory</p></li>
</ol>
</section>
<section id="dynamic-prompt-modification">
<h3><a class="toc-backref" href="#id17" role="doc-backlink">Dynamic Prompt Modification</a><a class="headerlink" href="#dynamic-prompt-modification" title="Link to this heading">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">is_stuck()</span></code> method in <code class="docutils literal notranslate"><span class="pre">BaseAgent</span></code> can modify the
<code class="docutils literal notranslate"><span class="pre">next_step_prompt</span></code> when stuck detection triggers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_stuck</span><span class="p">():</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">next_step_prompt</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;Your previous approaches didn&#39;t work. &quot;</span>
        <span class="s2">&quot;Try a completely different strategy.&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>This is the only runtime prompt modification mechanism.</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">OpenManus</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">System Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="agent_system.html">Agent System</a></li>
<li class="toctree-l1"><a class="reference internal" href="orchestration_flow.html">Orchestration Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="tool_system.html">Tool System</a></li>
<li class="toctree-l1"><a class="reference internal" href="knowledge_and_skills.html">Knowledge and Skills System</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">LLM Integration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#llm-client">LLM Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="#core-methods">Core Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="#message-formatting">Message Formatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#token-counting">Token Counting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#retry-logic">Retry Logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="#aws-bedrock-adapter">AWS Bedrock Adapter</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prompt-management">Prompt Management</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcp_integration.html">MCP Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="sandbox_system.html">Sandbox System</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_reference.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="knowledge_and_skills.html" title="previous chapter">Knowledge and Skills System</a></li>
      <li>Next: <a href="configuration.html" title="next chapter">Configuration</a></li>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, FoundationAgents.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="_sources/llm_integration.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>