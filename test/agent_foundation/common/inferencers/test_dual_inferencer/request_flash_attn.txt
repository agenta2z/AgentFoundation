Can you help carefully check if we could apply the following to model data/users/zgchen/fbs_8b028c_cfr_mo_dev/fbcode/minimal_viable_ai/models/main_feed_mtml/model_roo_v0.py or its dependencies, and help improve module efficiency during training/inference

"""
# Option 1: Explicit FA v3 import for H100
from moduler.flash_attn.hopper import flash_attn_func as flash_attn_v3

def attention_forward(q, k, v, is_h100=True):
    if is_h100:
        return flash_attn_v3(q, k, v, causal=True)
    return torch.nn.functional.scaled_dot_product_attention(q, k, v)

# Option 2: Backend selection hint (complements FlashGuard)
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False):
    attn_output = F.scaled_dot_product_attention(q, k, v)

"""

You need to create unit testing and a benchmark tool and run the benchmark tool to verify the code and performance improvement.
